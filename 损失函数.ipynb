{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是损失函数？\n",
    "\n",
    "损失函数(loss)是`预测值`和`正确答案`的差距。神经网络的目标是找到一组参数可以让`预测值`无限接近`正确答案`，即损失函数最小的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均方误差（mse）\n",
    "\n",
    "$$\n",
    "MSE(y_i, y) = \\frac{\\sum_{i=1}^n(y-y_i)^2}{n}\n",
    "$$\n",
    "\n",
    "```python\n",
    "loss_mse = tf.reduce_mean(tf.square(y_-y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习题\n",
    "目标：预测酸奶日销量y,影响酸奶日销量的因素包括了x1,x2\n",
    "\n",
    "数据：每日的x1,x2和当天的销量y_（正确答案）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 = [[-0.18593146]\n",
      " [ 0.03002513]]\n",
      "After 500 training steps, w1 = [[0.97760665]\n",
      " [1.0182234 ]]\n",
      "After 1000 training steps, w1 = [[1.0006421]\n",
      " [0.9980455]]\n",
      "After 1500 training steps, w1 = [[1.0038046]\n",
      " [0.9952709]]\n",
      "After 2000 training steps, w1 = [[1.0042382]\n",
      " [0.9948903]]\n",
      "After 2500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 3000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 3500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 4000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 4500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 5000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 5500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 6000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 6500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 7000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 7500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 8000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 8500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 9000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 9500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 10000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 10500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 11000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 11500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 12000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 12500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 13000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 13500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 14000 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n",
      "After 14500 training steps, w1 = [[1.004296  ]\n",
      " [0.99483895]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 23455\n",
    "\n",
    "rdm = np.random.RandomState(seed=SEED)\n",
    "x = rdm.rand(32, 2)\n",
    "y_ = [[x1 + x2 + (rdm.rand()/10.0-0.05)] for (x1, x2) in x]\n",
    "x = tf.cast(x, dtype=tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random.normal([2,1], stddev=1, mean=0, seed=1))\n",
    "\n",
    "epoch = 15000\n",
    "lr = 0.02\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = tf.matmul(x, w1)\n",
    "        loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "    grads = tape.gradient(loss_mse, w1)\n",
    "    w1.assign_sub(lr * grads)\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print('After {} training steps, w1 = {}'.format(epoch, w1.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出w1中的两个元素在向1靠近，与方程$y = x1 + x2$一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义损失函数\n",
    "\n",
    "使用均方误差作为损失函数预测酸奶销量，默认认为是预测多了和预测少了的时候损失都是一样的。\n",
    "\n",
    "但是真实的情况是，预测多了，损失成本；预测少了，损失利润。往往`利润`不等于`成本`。\n",
    "\n",
    "自定义损失函数：\n",
    "\n",
    "$$\n",
    "f(y_i,y) = \\begin{cases} PROFIT * (y_i - y) & y<y_i \\\\\\\\ COST*(y-y_i) & y>=y_i\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 = [[3.3923972]\n",
      " [1.3141794]]\n",
      "After 500 training steps, w1 = [[2.6693935]\n",
      " [2.8167872]]\n",
      "After 1000 training steps, w1 = [[1.4506109]\n",
      " [1.515564 ]]\n",
      "After 1500 training steps, w1 = [[3.8882358]\n",
      " [4.1180105]]\n",
      "After 2000 training steps, w1 = [[2.6694531]\n",
      " [2.8167872]]\n",
      "After 2500 training steps, w1 = [[1.4506705]\n",
      " [1.515564 ]]\n",
      "After 3000 training steps, w1 = [[3.8882954]\n",
      " [4.1180105]]\n",
      "After 3500 training steps, w1 = [[2.6695127]\n",
      " [2.8167872]]\n",
      "After 4000 training steps, w1 = [[1.4507301]\n",
      " [1.515564 ]]\n",
      "After 4500 training steps, w1 = [[3.888355 ]\n",
      " [4.1180105]]\n",
      "After 5000 training steps, w1 = [[2.6695724]\n",
      " [2.8167872]]\n",
      "After 5500 training steps, w1 = [[1.4507897]\n",
      " [1.515564 ]]\n",
      "After 6000 training steps, w1 = [[3.8884146]\n",
      " [4.1180105]]\n",
      "After 6500 training steps, w1 = [[2.669632 ]\n",
      " [2.8167872]]\n",
      "After 7000 training steps, w1 = [[1.4508493]\n",
      " [1.515564 ]]\n",
      "After 7500 training steps, w1 = [[3.8884742]\n",
      " [4.1180105]]\n",
      "After 8000 training steps, w1 = [[2.6696916]\n",
      " [2.8167872]]\n",
      "After 8500 training steps, w1 = [[1.4509089]\n",
      " [1.515564 ]]\n",
      "After 9000 training steps, w1 = [[3.8885338]\n",
      " [4.1180105]]\n",
      "After 9500 training steps, w1 = [[2.6697512]\n",
      " [2.8167872]]\n",
      "After 10000 training steps, w1 = [[1.4509685]\n",
      " [1.515564 ]]\n",
      "After 10500 training steps, w1 = [[3.8885934]\n",
      " [4.1180105]]\n",
      "After 11000 training steps, w1 = [[2.6698108]\n",
      " [2.8167872]]\n",
      "After 11500 training steps, w1 = [[1.4510281]\n",
      " [1.515564 ]]\n",
      "After 12000 training steps, w1 = [[3.888653 ]\n",
      " [4.1180105]]\n",
      "After 12500 training steps, w1 = [[2.6698704]\n",
      " [2.8167872]]\n",
      "After 13000 training steps, w1 = [[1.4510877]\n",
      " [1.515564 ]]\n",
      "After 13500 training steps, w1 = [[3.8887126]\n",
      " [4.1180105]]\n",
      "After 14000 training steps, w1 = [[2.66993  ]\n",
      " [2.8167872]]\n",
      "After 14500 training steps, w1 = [[1.4511473]\n",
      " [1.515564 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 23455\n",
    "\n",
    "rdm = np.random.RandomState(seed=SEED)\n",
    "x = rdm.rand(32, 2)\n",
    "y_ = [[x1 + x2 + (rdm.rand() / 10.0 - 0.05)] for (x1, x2) in x]\n",
    "x = tf.cast(x, dtype=tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random.normal([2, 1], stddev=1, mean=0, seed=1))\n",
    "\n",
    "epoch = 15000\n",
    "lr = 0.02\n",
    "COST = 2.0\n",
    "PROFIT = 10.0\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = tf.matmul(x, w1)\n",
    "        loss_mse = tf.reduce_sum(\n",
    "            tf.where(y > y_, COST * (y - y_), PROFIT * (y_ - y)))\n",
    "    grads = tape.gradient(loss_mse, w1)\n",
    "    w1.assign_sub(lr * grads)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print('After {} training steps, w1 = {}'.format(epoch, w1.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当利润>成本时，模型会往多的预测。\n",
    "\n",
    "当利润<成本时，模型会往少的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵\n",
    "\n",
    "交叉熵CE：表达了两个概率分布之间的距离。\n",
    "\n",
    "$$\n",
    "H(y_i, y) = - \\sum{y_i*\\ln y}\n",
    "$$\n",
    "\n",
    "例如：二分类问题，答案是$y_i = (1, 0)$，预测结果是$y_1 = (0.6, 0.4)$ $y_2 = (0.8, 0.2)$那么哪个更接近标准答案？\n",
    "\n",
    "$$\n",
    "H_1{((1,0), (0.6,0.4))} = -(1*\\ln{0.6} + 0*\\ln{0.4}) = 0.511\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_2{((1,0), (0.8,0.2))} = -(1*\\ln{0.8} + 0*\\ln{0.2}) = 0.223\n",
    "$$\n",
    "\n",
    "$H_1 > H_2$，所以$y_2$更加准确\n",
    "\n",
    "```python\n",
    "tf.losses.categorical_crossentropy(y_, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.7719568, shape=(), dtype=float32)\n",
      "tf.Tensor(2.9957323, shape=(), dtype=float32)\n",
      "tf.Tensor(0.38566247, shape=(), dtype=float32)\n",
      "tf.Tensor(0.07257068, shape=(), dtype=float32)\n",
      "tf.Tensor(0.07257068, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "loss_ce1 = tf.losses.categorical_crossentropy([0, 0, 1], [0.51, 0.32, 0.17])\n",
    "loss_ce2 = tf.losses.categorical_crossentropy([0, 0, 1], [0.22, 0.73, 0.05])\n",
    "loss_ce3 = tf.losses.categorical_crossentropy([0, 0, 1], [0.15, 0.17, 0.68])\n",
    "loss_ce4 = tf.losses.categorical_crossentropy([0, 0, 1], [0.04, 0.03, 0.93])\n",
    "print(loss_ce1)\n",
    "print(loss_ce2)\n",
    "print(loss_ce3)\n",
    "print(loss_ce4)\n",
    "print(tf.reduce_min([loss_ce1, loss_ce2, loss_ce3, loss_ce4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
