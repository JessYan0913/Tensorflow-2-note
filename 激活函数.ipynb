{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预备知识\n",
    "\n",
    "### `tf.where`\n",
    "\n",
    "`tf.where(判别条件,A,B)`是`tensorflow`中的条件语句成了返回A，否则返回B\n",
    "\n",
    "```python\n",
    "tf.where(?, A, B)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 3, 4, 1, 1], dtype=int32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([1,2,3,1,1])\n",
    "b = tf.constant([0,3,4,1,0])\n",
    "tf.where(a > b, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `np.random.RandomState.rand`\n",
    "\n",
    "`np.random.RandomState.rand(维度)`可以返回一个[0,1)的随机数，若是维度为空则返回的是一个标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.417022004702574"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rdm = np.random.RandomState(seed=1) #设置随机种子，保证每次生成的随机数相同\n",
    "a = rdm.rand()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.20324493e-01, 1.14374817e-04, 3.02332573e-01],\n",
       "       [1.46755891e-01, 9.23385948e-02, 1.86260211e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = rdm.rand(2,3) #返回一个2行3列的随机数矩阵\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `np.vstack`\n",
    "\n",
    "`np.vstack(数组1, 数组2)`可以将两个数组拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([3,4,5])\n",
    "np.vstack((a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `np.mgrid、np.ravel、np.c_`生成网格坐标点\n",
    "\n",
    "`np.mgrid[起始值:结束值:步长, 起始值:结束值:步长, ...]`生成指定范围的矩阵，可以同时生成多个矩阵，用`,`隔开。\n",
    "\n",
    "`x.ravel()`将x拉伸到一维数组\n",
    "\n",
    "`np.c_[数组1,数组2,....]`使得返回的间隔数值点配对，可以配对多个数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2.]]\n",
      "[[3.  3.4 3.8 4.2 4.6 5.  5.4 5.8]\n",
      " [3.  3.4 3.8 4.2 4.6 5.  5.4 5.8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x,y = np.mgrid[1:3:1, 3:6:0.4]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "[3.  3.4 3.8 4.2 4.6 5.  5.4 5.8 3.  3.4 3.8 4.2 4.6 5.  5.4 5.8]\n"
     ]
    }
   ],
   "source": [
    "x = x.ravel()\n",
    "y = y.ravel()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  3. ]\n",
      " [1.  3.4]\n",
      " [1.  3.8]\n",
      " [1.  4.2]\n",
      " [1.  4.6]\n",
      " [1.  5. ]\n",
      " [1.  5.4]\n",
      " [1.  5.8]\n",
      " [2.  3. ]\n",
      " [2.  3.4]\n",
      " [2.  3.8]\n",
      " [2.  4.2]\n",
      " [2.  4.6]\n",
      " [2.  5. ]\n",
      " [2.  5.4]\n",
      " [2.  5.8]]\n"
     ]
    }
   ],
   "source": [
    "grid = np.c_[x, y]\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数\n",
    "\n",
    "激活函数可以增加模型的表达度，因为$y = xw + b$是一个线性函数，即使有多层神经元首尾相接构成深层网络，但依旧是线性模型，模型的表达度收到限制，所以加入`非线性函数`的激活函数使得模型不再是输入`x`的线性组合，增加了模型表达度。\n",
    "\n",
    "### 好的激活函数\n",
    "- **非线性**：激活函数非线性时，多层神经网络可逼近所有函数\n",
    "- **可微分**：优化器大多使用梯度下降更新参数，如果激活函数不可微那么就无法更新参数\n",
    "- **单调性**：当激活函数是单调的，能保证单层网络的损失函数是凸函数，可收敛\n",
    "- **近似恒等**：$f(x) \\approx x$也就是激活函数的输出值应当约等于输入值，当参数初始化为随机小值时，神经网络更稳定\n",
    "\n",
    "### 激活函数输出值的范围\n",
    "- 激活函数输出为`有限值`时（分类问题），基于梯度的优化方法更稳定\n",
    "- 激活函数输出为`无限值`时（回归问题），建议调小学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sigmoid`\n",
    "\n",
    "`tf.nn.sigmoid(x)`激活函数可以将输出限制在0-1之间。\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "特点是：\n",
    "- 容易造成梯度消失：因为`sigmoid`函数的导数在0-0.25之间，在神经网络中更新参数时会采用逐层求导让后相乘，若网络的激活函数是`sigmoid`就会有多个0-0.25之间的数相乘，使得结果无限趋近于零，造成了梯度消失，网络参数无法更新。\n",
    "- 输出非0均值，收敛慢\n",
    "- 存在幂运算，计算时间长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tanh`\n",
    "\n",
    "`tf.math.tanh(x)`激活函数可以将输出均值限制在0附近。\n",
    "$$\n",
    "f(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}\n",
    "$$\n",
    "\n",
    "特点是：\n",
    "- 输出均值是0\n",
    "- 容易造成梯度消失：原因和`sigmoid`一样\n",
    "- 存在幂运算，计算时间长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `relu`\n",
    "\n",
    "`tf.nn.relu()`激活函数是一个分段函数，输入小于0时的输出都是0。\n",
    "$$\n",
    "f(x) = max(x,0) = \\begin{cases} 0 & x<0 \\\\\\\\ x & x>=0\\end{cases}\n",
    "$$\n",
    "优点：\n",
    "- 在正区间时，解决了梯度消失\n",
    "- 只需要判断输入是否大于0，计算快\n",
    "- 收敛速度快`sigmoid`\n",
    "\n",
    "缺点：\n",
    "- 输出的均值不是0，收敛慢\n",
    "- `Dead RelU`问题：当输入存在过多负数时，输出为0，反向传播的梯度就是0，参数无法更新，造成神经元死亡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Leaky Relu`\n",
    "\n",
    "`tf.nn.leaky_relu(x)`可以解决`relu`输入为负数时，输出为0，造成神经元死亡的问题。\n",
    "$$\n",
    "f(x) = max(ax,x)\n",
    "$$\n",
    "\n",
    "`leaky_relu`在负区间的输出不再是0。拥有`relu`的所有优点，还解决了`Dead Relu`问题。但在实际操作中还是经常选择`relu`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "- 首选`relu`函数\n",
    "- 学习率设置比较小最好\n",
    "- 输入特征标准化，即让输入数据集的均值是0，标准差是1，的正态分布\n",
    "- 初始参数中心话，即让随机生成的参数满足均值是0，标准差是$\\sqrt{\\frac{2}{当前层输入特征数}}$的正态分布"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
