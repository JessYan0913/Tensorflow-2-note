{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.卷积神经网络\n",
    "## 1.1.卷积\n",
    "\n",
    "利用卷积核提取特征，卷积核中的参数与特征像素点相乘后再相加输出为新的特征。\n",
    "\n",
    "### Tensorflow描述\n",
    "\n",
    "```py\n",
    "tk.keras.layers.Conv2D(\n",
    "filter=卷积核个数,\n",
    "kernel_size=卷积核尺寸, #正方形写核长整数，或（核高h, 核宽w）\n",
    "strides=滑动步长, #横纵方向相同步长写整数，或（纵向步长h, 横向步长w）默认1\n",
    "padding='same'or'valid', #使用全零填充是'same'，不使用是'valid'(默认)\n",
    "activation='relu'or'sigmoid'or'tanh'or'softmax', #如果有BN层，此处不写\n",
    "input_shape=(高,宽,通道数) #输入特征图的维度，可以省略\n",
    ")\n",
    "```\n",
    "\n",
    "使用示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "\n",
    "model = models.Sequential([\n",
    "    Conv2D(filters=6, kernel_size=5, padding='valid', activation='sigmoid'),\n",
    "    MaxPool2D(2,2),\n",
    "    Conv2D(filters=6, kernel_size=(5,5), padding='valid', activation='sigmoid'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Conv2D(filters=6, kernel_size=(5,5), padding='valid', activation='sigmoid'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.批标准化 (Batch Normalization BN)\n",
    "标准化：使数据符合均值为0，标准差为1的正态分布。\n",
    "\n",
    "批标准化：对一小批（一个batch）的数据，做标准化。\n",
    "\n",
    "批标准化后，第K个卷积核的输出特征图中第i个像素点：\n",
    "$$\n",
    "H_i^{'k} = \\frac{H_i^k - u_{batch}^k}{r_{batch}^k}\n",
    "$$\n",
    "\n",
    "- $H_i^k$：批标准化之前，第K个卷积核，输出特征图中第i个像素点\n",
    "- $u_{batch}^k$：批标准化之前，第K个卷积核，batch张输出特征图中所有像素点的平均值\n",
    "- $r_{batch}^k$：批标准化之前，第K个卷积核，batch张输出特征图中所有像素点的标准差\n",
    "\n",
    "批标准化操作，将原本偏移的特征数据，重新拉回到0均值，使得特征分布在激活函数的线性区域内，使得输入数据的微小变化更明显的体现到激活函数的输出上。BN操作提升了激活函数对输入数据的区分能力。\n",
    "\n",
    "但是简单的批标准化$H_i^{'k} = \\frac{H_i^k - u_{batch}^k}{r_{batch}^k}$，使得输入数据集中在激活函数中心的线性区域中，使激活函数丧失了非线性特性，因此在BN操作时，为每个卷积核引入两个可训练参数：\n",
    "$$\n",
    "X_i^k = v_kH_i^{'k} + b_k\n",
    "$$\n",
    "\n",
    "- $v_k$：缩放因子\n",
    "- $b_k$：偏移因子\n",
    "\n",
    "在反向传播时，$v_k$和$b_k$会和其他带训练参数一同被训练优化，使标准正态分布后的特征数据，通过缩放因子和偏移因子，优化了特征分布的宽度和偏移量。保证了激活函数的非线性能力。\n",
    "\n",
    "### BN操作的位置\n",
    "BN层位于卷积层之后，在激活层之前\n",
    "\n",
    "### Tensorflow描述\n",
    "\n",
    "```py\n",
    "tf.keras.layers.BatchNormalization()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    Conv2D(filters=6, kernel_size=(5,5), padding='same'), #卷积\n",
    "    BatchNormalization(), #批标准化\n",
    "    Activation('relu'), #激活层\n",
    "    MaxPool2D(pool_size=(2,2), strides=2, padding='same'), #池化层\n",
    "    Dropout(0.2) #dropout层\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.池化\n",
    "\n",
    "池化操作的目的：在保留特征信息的前提下，减少特征的数据量。\n",
    "\n",
    "常见的池化操作有：\n",
    "- 最大池化：即输出操作核中的最大像素值，用于提取特征图片的纹理\n",
    "- 均值池化：即输出操作和中的像素平均值，用于保留背景特征\n",
    "\n",
    "### Tensorflow描述\n",
    "\n",
    "```py\n",
    "tf.keras.layers.MaxPool2D(\n",
    "pool_size=池化核尺寸, #正方形写核长整数，或（核高h, 核宽w）\n",
    "strides=池化步长, #步长整数，或（纵向步长h，横向步长w）默认是pool_size\n",
    "padding='valid'or 'same' #使用全零填充'same'，不使用的话'valid'\n",
    ")\n",
    "\n",
    "tf.keras.layers.AveragePooling2D(\n",
    "pool_size=池化核尺寸, #正方形写核长整数，或（核高h, 核宽w）\n",
    "strides=池化步长, #步长整数，或（纵向步长h，横向步长w）默认是pool_size\n",
    "padding='valid'or 'same' #使用全零填充'same'，不使用的话'valid'\n",
    ")\n",
    "```\n",
    "使用示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    Conv2D(filters=6, kernel_size=(5,5), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2, padding='same'),\n",
    "    Dropout(.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.舍弃 (Dropout)\n",
    "\n",
    "在神经网络训练时，将一部分神经元按照一定概率从神经网络中暂时舍弃。当神经网络使用时，被舍弃的神经元恢复链接。\n",
    "\n",
    "### Tensorflow描述\n",
    "\n",
    "```py\n",
    "tf.keras.layers.Dropout(舍弃概率)\n",
    "```\n",
    "\n",
    "使用示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    Conv2D(filters=6, kernel_size=(5,5), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2, padding='same'),\n",
    "    Dropout(.2) #每个神经元被舍弃的概率为20%\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.小结\n",
    "\n",
    "卷积神经网络，是借助卷积核提取特征后，将提取的特征输入全连接网络\n",
    "\n",
    "### 卷积神经网络的主要模块\n",
    "\n",
    "卷积(convolutional) -> 批标准化(BN) -> 激活(Activation) -> 池化(Pooling) -> 全连接\n",
    "\n",
    "提取特征包括：卷积(convolutional)、批标准化(Batch Normalization BN)、激活(Activation)、池化(Pooling)\n",
    "\n",
    "### 卷积是什么：\n",
    "\n",
    "卷积就是特征提取器，就是C(Convolutional),B(Batch Normalization),A(Activation),P(Pooling),D(Dropout)\n",
    "\n",
    "```py\n",
    "model = models.Sequential([\n",
    "    Conv2D(filters=6, kernel_size=(5,5), padding='same'), # C\n",
    "    BatchNormalization(),                                 # B\n",
    "    Activation('relu'),                                   # A\n",
    "    MaxPool2D(pool_size=(2,2), strides=2, padding='same'),# P \n",
    "    Dropout(.2)                                           # D\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.卷积神经网络搭建示例\n",
    "\n",
    "## 2.1.使用卷积神经网络训练 cifar10数据集\n",
    "网络结构为：\n",
    "- 5x5的卷积核，个数6\n",
    "- 2x2的池化核，步长2\n",
    "- 128个神经元的全连接\n",
    "- 10个神经元的全连接\n",
    "\n",
    "搭建：\n",
    "- C(核:6*5*5, 步长:1, 填充:same)\n",
    "- B(Yes)\n",
    "- A(relu)\n",
    "- P(max, 核:2*2, 步长:2, 填充:same)\n",
    "- D(0.2)\n",
    "- Flatten\n",
    "- Dense(神经元:128, 激活:relu, Dropout:0.2)\n",
    "- Dense(神经元:10, 激活:softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "\n",
    "class Baseline(Model):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.c1 = Conv2D(filters=6, kernel_size=(5,5), padding='same')\n",
    "        self.b1 = BatchNormalization()\n",
    "        self.a1 = Activation('relu')\n",
    "        self.p1 = MaxPool2D(pool_size=(2,2), strides=2, padding='same')\n",
    "        self.d1 = Dropout(.2)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dropout(.2)\n",
    "        self.f2 = Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.d2(x)\n",
    "        y = self.f2(x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.LeNet卷积网络实现\n",
    "\n",
    "LeNet是LeCun在1998年提出的，是卷积网络的开篇之作。LeNet通过共享卷积核减少网络参数。卷积神经网络层数一般只统计卷积计算层和全连接计算层，其余操作认为是附属操作。\n",
    "\n",
    "![lenet.jpeg](./image/lenet.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：32*32的图片\n",
    "\n",
    "卷积1:\n",
    "- C(核:6x5x5, 步长:1, 填充:valid)\n",
    "- B(None)\n",
    "- A(sigmoid)\n",
    "- P(maxPooling, 核:2*2, 步长:2, 填充:valid)\n",
    "- D(None)\n",
    "\n",
    "卷积2:\n",
    "- C(核:16x5x5, 步长:1, 填充:valid)\n",
    "- B(None)\n",
    "- A(sigmoid)\n",
    "- P(maxPooling, 核:2*2, 步长:2, 填充:valid)\n",
    "- D(None)\n",
    "\n",
    "全连接:\n",
    "- Flatten\n",
    "- Dense(神经元:128, 激活:sigmoid)\n",
    "- Dense(神经元:84, 激活:sigmoid)\n",
    "- Dense(神经元:10, 激活:softmax)\n",
    "\n",
    "LeNet代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "\n",
    "class LeNet5(Model):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__\n",
    "        self.c1 = Conv2D(filters=6, kernel_size=(5,5), activation='sigmoid')\n",
    "        self.p1 = MaxPool2D(pool_size=(2,2), strides=2)\n",
    "        self.c2 = Conv2D(filters=16, kernel_size=(5,5), activation='sigmoid')\n",
    "        self.p2 = MaxPool2D(pool_size=(2,2), strides=2)\n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(128, activation='sigmoid')\n",
    "        self.f2 = Dense(84, activation='sigmoid')\n",
    "        self.f3 = Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        y = self.f3(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.AlexNet\n",
    "\n",
    "AlexNet是2012年的ImageNet冠军，Top5错误率16.4%。使用了'relu'激活函数提升训练速度，使用'dropout'缓解过拟合。\n",
    "\n",
    "输入：32x32x3的图片\n",
    "\n",
    "卷积1:\n",
    "- C(核: 96x3x3, 步长:1, 填充:valid)\n",
    "- B(Yes, LRN/BN)\n",
    "- A(relu)\n",
    "- P(max, 核:3x3, 步长:2)\n",
    "- D(None)\n",
    "\n",
    "卷积2:\n",
    "- C(核: 256x3x3, 步长:1, 填充:valid)\n",
    "- B(Yes, LRN/BN)\n",
    "- A(relu)\n",
    "- P(max, 核:3x3, 步长:2)\n",
    "- D(None)\n",
    "\n",
    "卷积3:\n",
    "- C(核: 384x3x3, 步长:1, 填充:same)\n",
    "- B(None)\n",
    "- A(relu)\n",
    "- P(None)\n",
    "- D(None)\n",
    "\n",
    "卷积4:\n",
    "- C(核: 384x3x3, 步长:1, 填充:same)\n",
    "- B(None)\n",
    "- A(relu)\n",
    "- P(None)\n",
    "- D(None)\n",
    "\n",
    "卷积5:\n",
    "- C(核: 256x3x3, 步长:1, 填充:same)\n",
    "- B(None)\n",
    "- A(relu)\n",
    "- P(max, 核:3x3, 步长:2)\n",
    "- D(None)\n",
    "\n",
    "全连接:\n",
    "- Flatten\n",
    "- Dense(神经元: 2048, 激活:relu, Dropout:0.5)\n",
    "- Dense(神经元: 2048, 激活:relu, Dropout:0.5)\n",
    "- Dense(神经元: 10, 激活:softmax)\n",
    "\n",
    "AlexNex代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "\n",
    "class AlexNet8(Model):\n",
    "    def __init__(self):\n",
    "        super(AlexNet8, self).__init__()\n",
    "        self.c1 = Conv2D(filters=96, kernel_size=(3,3))\n",
    "        self.b1 = BatchNormalization()\n",
    "        self.a1 = Activation('relu')\n",
    "        self.p1 = MaxPool2D(pool_size=(3,3), strides=2)\n",
    "        self.c2 = Conv2D(filters=256, kernel_size=(3,3))\n",
    "        self.b2 = BatchNormalization()\n",
    "        self.a2 = Activation('relu')\n",
    "        self.p2 = MaxPool2D(pool_size=(3,3), strides=2)\n",
    "        self.c3 = Conv2D(filters=384, kernel_size=(3,3), padding='same')\n",
    "        self.a3 = Activation('relu')\n",
    "        self.c4 = Conv2D(filters=384, kernel_size=(3,3), padding='same')\n",
    "        self.a4 = Activation('relu')\n",
    "        self.c5 = Conv2D(filters=256, kernel_size=(3,3), padding='same')\n",
    "        self.a5 = Activation('relu')\n",
    "        self.p5 = MaxPool2D(pool_size=(3,3), stides=2)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(2048, activation='relu')\n",
    "        self.d1 = Dropout(0.5)\n",
    "        self.f2 = Dense(2048, activation='relu')\n",
    "        self.d2 = Dropout(0.5)\n",
    "        self.f3 = Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "        \n",
    "        x = self.c2(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.p2(x)\n",
    "        \n",
    "        x = self.c3(x)\n",
    "        x = self.a3(x)\n",
    "        \n",
    "        x = self.c4(x)\n",
    "        x = self.a4(x)\n",
    "        \n",
    "        x = self.c5(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.p5(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.d2(x)\n",
    "        y = self.f3(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
