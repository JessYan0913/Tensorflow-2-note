{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待优化的参数$w$\n",
    "\n",
    "损失函数$loss$\n",
    "\n",
    "学习率$lr$\n",
    "\n",
    "每次迭代一个$batch$\n",
    "\n",
    "$t$表示当前$batch$迭代的总次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新参数步骤\n",
    "\n",
    "- 计算$t$时刻损失函数关于当前参数的地图$g_t = \\frac{\\partial{loss}}{\\partial{(w_t)}}$\n",
    "- 计算$t$时刻一阶动量$m_t$和二阶动量$V_t$，一阶动量是与梯度相关的函数，二阶动量是与梯度平方相关的函数\n",
    "- 计算$t$时刻下降梯度：$\\eta_t = \\frac{lr.m_t}{\\sqrt{V_t}}$\n",
    "- 计算$t+1$时刻参数：$w_{t+1} = w_t - \\frac{lr·m_t}{\\sqrt{V_t}}$\n",
    "\n",
    ">不同的优化器的区别是，一阶动量和二阶动量的函数不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD梯度下降法\n",
    "\n",
    "$$\n",
    "m_t = g_t \\\\\\\\ V_t = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta_t = lr·m_t/\\sqrt{V_t} = lr · g_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta_t = w_t - lr·m_t/\\sqrt{V_t} = w_t - lr·g_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是 $w_{t+1} = w_t - lr * \\frac{\\partial{loss}}{\\partial{w_t}}$\n",
    "\n",
    "```python\n",
    "w1.assign_sub(lr * grads[0])\n",
    "b1.assign_sub(lr * grads[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDM 在SGD的基础上增加了一阶动量\n",
    "\n",
    "$$\n",
    "m_t = \\beta·m_{t-1} + (1-\\beta)·g_t \\\\ V_t = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta_t = lr·m_t/\\sqrt{V_t} = lr · m_t = lr · (\\beta·m_{t-1} + (1-\\beta)·g_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta_t = w_t - lr·(\\beta·m_{t-1} + (1-\\beta)·g_t)\n",
    "$$\n",
    "\n",
    "代码中描述：\n",
    "\n",
    "```python\n",
    "m_w, m_b = 0, 0\n",
    "beta = 0.9\n",
    "\n",
    "···\n",
    "\n",
    "grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "##########################################################################\n",
    "# sgd-momentun  \n",
    "m_w = beta * m_w + (1 - beta) * grads[0]\n",
    "m_b = beta * m_b + (1 - beta) * grads[1]\n",
    "w1.assign_sub(lr * m_w)\n",
    "b1.assign_sub(lr * m_b)\n",
    "\n",
    "···\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad 在SGD的基础上增加了二阶动量\n",
    "\n",
    "$$\n",
    "m_t = g_t \\\\ V_t = \\sum_{\\tau_1}^tg_{\\tau}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta_t = lr·m_t/\\sqrt{V_t} = lr · g_t /\\sqrt{\\sum_{\\tau_1}^tg_{\\tau}^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta_t = w_t - lr · g_t /\\sqrt{\\sum_{\\tau_1}^tg_{\\tau}^2}\n",
    "$$\n",
    "\n",
    "代码中描述：\n",
    "\n",
    "```python\n",
    "v_w, v_b = 0, 0\n",
    "\n",
    "# 计算loss对各个参数的梯度\n",
    "grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "##########################################################################\n",
    "# adagrad\n",
    "v_w += tf.square(grads[0])\n",
    "v_b += tf.square(grads[1])\n",
    "w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))\n",
    "b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp 在SGD的基础上增加了二阶动量\n",
    "\n",
    "$$\n",
    "m_t = g_t \\\\ V_t = \\beta·V_{t-1} + (1 - \\beta)·g_t^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta_t = lr·m_t/\\sqrt{V_t} = lr · g_t /\\sqrt{\\beta·V_{t-1} + (1 - \\beta)·g_t^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta_t = w_t - lr · g_t /\\sqrt{\\beta·V_{t-1} + (1 - \\beta)·g_t^2}\n",
    "$$\n",
    "\n",
    "代码中描述：\n",
    "\n",
    "```python\n",
    "v_w, v_b = 0, 0\n",
    "beta = 0.9\n",
    "\n",
    "# 计算loss对各个参数的梯度\n",
    "grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "##########################################################################\n",
    "# rmsprop\n",
    "v_w = beta * v_w + (1 - beta) * tf.square(grads[0])\n",
    "v_b = beta * v_b + (1 - beta) * tf.square(grads[1])\n",
    "w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))\n",
    "b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam 结合SGDM的一阶动量和RMSProp的二阶动量\n",
    "\n",
    "```python\n",
    "m_w, m_b = 0, 0\n",
    "v_w, v_b = 0, 0\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "delta_w, delta_b = 0, 0\n",
    "global_step = 0\n",
    "\n",
    "# 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "##########################################################################\n",
    " # adam\n",
    "m_w = beta1 * m_w + (1 - beta1) * grads[0]\n",
    "m_b = beta1 * m_b + (1 - beta1) * grads[1]\n",
    "v_w = beta2 * v_w + (1 - beta2) * tf.square(grads[0])\n",
    "v_b = beta2 * v_b + (1 - beta2) * tf.square(grads[1])\n",
    "\n",
    "m_w_correction = m_w / (1 - tf.pow(beta1, int(global_step)))\n",
    "m_b_correction = m_b / (1 - tf.pow(beta1, int(global_step)))\n",
    "v_w_correction = v_w / (1 - tf.pow(beta2, int(global_step)))\n",
    "v_b_correction = v_b / (1 - tf.pow(beta2, int(global_step)))\n",
    "\n",
    "w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))\n",
    "b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
